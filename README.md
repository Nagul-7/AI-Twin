# AI Twin (Local RAG)

This project builds a **local AI Twin** using:
- Ollama (local LLM runner)
- LangChain (LLM orchestration)
- ChromaDB (vector database for RAG)

No cloud APIs. Everything runs locally on Kali Linux.

---

## ğŸ“ Project Structure

ai-twin/
â”œâ”€â”€ ingest.py        # Converts documents into embeddings and stores them in Chroma
â”œâ”€â”€ query.py         # Ask questions to the local LLM using RAG
â”œâ”€â”€ prompts.py       # Prompt templates for the AI
â”œâ”€â”€ data/            # Documents used for knowledge (PDF, TXT, etc.)
â”œâ”€â”€ vectordb/        # Persistent vector database (Chroma)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md        # Project documentation

---

## âš™ï¸ Setup Instructions

### 1. Create virtual environment
```bash
python3 -m venv venv
source venv/bin/activate


3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Start Ollama
ollama serve

ğŸ“¥ Index Documents (RAG)

Place your documents inside the data/ folder, then run:

python ingest.py

ğŸ’¬ Query the AI Twin
python query.py


Type exit to quit.

ğŸ§  How It Works

Documents are chunked and embedded using nomic-embed-text

Vectors are stored in ChromaDB

Queries retrieve relevant chunks

Context is passed to a local LLM via Ollama

The model answers only using retrieved context

